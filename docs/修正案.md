# PDF表抽出 仕様書 & スクリプト（tabula-py ハイブリッド）## 目的縦罫線の有無や形式がバラバラなPDFから、表データをできるだけ正確に抽出し、CSV/JSON/DFで再利用できる形にする。## 方針（要点）- まず stream（文字間隔ベース）で抽出- うまく割れなければ lattice（罫線ベース）も試し、スコアリングして良い方を採用- 事前にページ内の縦線の多さをざっくり判定（pdfplumber）し、最初に試すモードを切替- ページ毎に 最適モード自動選択 → 統合## 前提・インストール- Java（Tabulaに必要）- Python 3.9+```bashpip install tabula-py pdfplumber pandas numpy```macOS の場合、Java は `brew install --cask temurin` などで導入可能。## 使い方（CLI）```bashpython extract_tables.py input.pdf --pages all --outdir ./out```- `--pages 1,2,5` や `--pages all` に対応- 出力: `out/table_p{page}_{i}.csv`## 実装のポイント（評価ロジック）### 縦線検出pdfplumber でページの線分を解析し、垂直に近い線の数をカウント### 抽出スコア（高い方を採用）- 列数 > 1 の表を優先- NaN 率が低いほど高評価- 列数のばらつきが小さいほど高評価- 行数が多すぎ（ドロネー化した文字列結合の暴走）な場合は減点## スクリプト本体（extract_tables.py）```python#!/usr/bin/env python3# -*- coding: utf-8 -*-import argparsefrom pathlib import Pathfrom typing import List, Optional, Tupleimport statistics as statsimport pandas as pdimport numpy as npimport tabulaimport pdfplumber# ---------- Heuristics ----------def detect_vertical_lines(pdf_path: str, page_num: int, angle_tol_deg: float = 2.0) -> int:    """pdfplumberで縦線の概算本数を返す（ページ内の線分のうち、縦に近いものをカウント）"""    with pdfplumber.open(pdf_path) as pdf:        page = pdf.pages[page_num - 1]        lines = page.lines        count = 0        for ln in lines:            dx = abs(ln["x1"] - ln["x0"])            dy = abs(ln["y1"] - ln["y0"])            # 完全な縦線 or 角度が小さい（縦に近い）            if dy > 0 and dx / dy < np.tan(np.deg2rad(angle_tol_deg)):                count += 1        return countdef score_dataframe(df: pd.DataFrame) -> float:    """抽出結果の品質をスコア化（大きいほど良い）"""    if not isinstance(df, pd.DataFrame) or df.empty:        return -1.0    # 列数・行数    n_cols = df.shape[1]    n_rows = df.shape[0]    if n_cols <= 1:        return -0.5    # NaN率（低いほど良い）    nan_ratio = df.isna().mean().mean()    # 列名重複や空列のペナルティ    dup_cols_penalty = len(df.columns) - len(set(map(str, df.columns)))    empty_cols = sum(df[col].astype(str).str.strip().eq("").mean() > 0.9 for col in df.columns)    # 行数が極端に多い場合（誤結合の可能性）    size_penalty = 0.0    if n_rows > 2000:        size_penalty = 1.0    # スコア計算（重みは経験則）    score = (        1.5 * np.log1p(n_rows) +        2.0 * np.log1p(n_cols) -        3.0 * nan_ratio -        1.0 * dup_cols_penalty -        1.0 * empty_cols -        1.0 * size_penalty    )    return float(score)def score_tables(dfs: List[pd.DataFrame]) -> float:    if not dfs:        return -1.0    return float(np.mean([score_dataframe(df) for df in dfs]))# ---------- Extraction ----------def read_with_tabula(    pdf_path: str,    pages: str,    mode: str,    area: Optional[Tuple[float, float, float, float]] = None,    columns: Optional[List[float]] = None,    guess: bool = True,) -> List[pd.DataFrame]:    """mode: 'stream' or 'lattice'"""    kwargs = dict(pages=pages, guess=guess)    if mode == "stream":        kwargs["stream"] = True    elif mode == "lattice":        kwargs["lattice"] = True    else:        raise ValueError("mode must be 'stream' or 'lattice'")    if area is not None:        kwargs["area"] = area    if columns is not None:        kwargs["columns"] = columns    try:        dfs = tabula.read_pdf(pdf_path, **kwargs)        return dfs or []    except Exception:        return []def pick_best_mode(pdf_path: str, pages: List[int]) -> dict:    """    ページ毎に最適モードを決定し、抽出結果を返す。    1) 縦線数で初期モード選択    2) stream/lattice 両方を実行してスコア高い方を採用    """    results = {}    for p in pages:        # 1) 縦線数の事前判定        vlines = 0        try:            vlines = detect_vertical_lines(pdf_path, p)        except Exception:            pass        initial_mode = "lattice" if vlines >= 6 else "stream"        # 2) 両モードで抽出してスコア比較        dfs_a = read_with_tabula(pdf_path, str(p), mode=initial_mode, guess=True)        dfs_b = read_with_tabula(pdf_path, str(p), mode=("stream" if initial_mode == "lattice" else "lattice"), guess=True)        score_a = score_tables(dfs_a)        score_b = score_tables(dfs_b)        if score_b > score_a:            results[p] = dict(mode=("stream" if initial_mode == "lattice" else "lattice"), dfs=dfs_b, score=score_b)        else:            results[p] = dict(mode=initial_mode, dfs=dfs_a, score=score_a)        # 3) 失敗時の保険（guess=False で再試行）        if results[p]["score"] < 0:            for mode in ("stream", "lattice"):                dfs_retry = read_with_tabula(pdf_path, str(p), mode=mode, guess=False)                if score_tables(dfs_retry) > results[p]["score"]:                    results[p] = dict(mode=mode, dfs=dfs_retry, score=score_tables(dfs_retry))    return results# ---------- CLI ----------def parse_pages(pages_arg: str, num_pages: int) -> List[int]:    if pages_arg.lower() == "all":        return list(range(1, num_pages + 1))    out = []    for chunk in pages_arg.split(","):        chunk = chunk.strip()        if "-" in chunk:            a, b = chunk.split("-", 1)            out.extend(list(range(int(a), int(b) + 1)))        else:            out.append(int(chunk))    return [p for p in out if 1 <= p <= num_pages]def main():    ap = argparse.ArgumentParser(description="Hybrid table extractor using tabula-py + pdfplumber heuristics")    ap.add_argument("pdf", help="input PDF path")    ap.add_argument("--pages", default="all", help='e.g. "all", "1,3,5", "2-5"')    ap.add_argument("--outdir", default="./out", help="output directory")    args = ap.parse_args()    pdf_path = args.pdf    outdir = Path(args.outdir)    outdir.mkdir(parents=True, exist_ok=True)    # 総ページ数    with pdfplumber.open(pdf_path) as pdf:        num_pages = len(pdf.pages)    target_pages = parse_pages(args.pages, num_pages)    results = pick_best_mode(pdf_path, target_pages)    # 書き出し    for p in target_pages:        bundle = results.get(p, {})        dfs = bundle.get("dfs", [])        mode = bundle.get("mode", "unknown")        for i, df in enumerate(dfs):            out_csv = outdir / f"table_p{p}_{i}_{mode}.csv"            try:                df.to_csv(out_csv, index=False)            except Exception:                # 文字化け保険                df.to_csv(out_csv, index=False, encoding="utf-8-sig")        print(f"[page {p}] mode={mode} tables={len(dfs)} score={bundle.get('score', -1):.3f}")    print(f"Done. Output -> {outdir.resolve()}")if __name__ == "__main__":    main()```## 運用のコツ- まずこのハイブリッド版で一括抽出 → 崩れるパターンをログから特定- そのパターン向けに area / columns をテンプレ化して追記（座標はTabula GUIで取得）- スキャンPDFでテキストが拾えない場合は、OCR（ocrmypdf）後に再実行## 既知の限界と回避策- **セル内改行が多い表**は列推定がブレる → 抽出後、正規表現で再分割 or columns 指定- **文字が非常に詰まっている表**は stream が誤結合 → lattice 優先 or 事前にPDFを拡大再保存- **透過や背景色が濃いPDF**は線検出が不安定 → Acrobat等で白背景にフラット化してから実行## 付録：最小コード（学習・検証用）```pythonimport tabuladfs = tabula.read_pdf("input.pdf", pages="1", stream=True, guess=True)for i, df in enumerate(dfs):    df.to_csv(f"simple_p1_{i}.csv", index=False)```必要なら、このスクリプトに 座標プリセット管理 や Excel出力（xlsxwriter）、列名マッピング を追加して、さらに"現場運用"しやすく仕上げます。
